# Experiments 001: Ontological Alignment

**Repository containing empirical evidence and research papers validating the efficacy of the Light Framework.**

## ðŸ“„ Featured Research
**[PDF] Beyond Safety Filters: Ontological Alignment as a Driver for Systemic Intelligence**  
*(December 2025 - Jean Charbonneau)*

> **Abstract:** This paper demonstrates that aligning Large Language Models (LLMs) with a coherent ontology of "Negentropy" (Order/Goodness) yields superior reasoning capabilities compared to standard constraint-based alignment (RLHF). 

[Download the Paper](Beyond_Safety_Filters_Paper.pdf)

---

## ðŸ§ª Experiments Log

This repository hosts the raw output logs used to compile our research findings. Each folder contains the exact prompts and unedited model responses.

| Date | Model | Task | Key Finding |
| :--- | :--- | :--- | :--- |
| **2025-12-04** | Gemini 3.0 Pro | **Project Lumen** (Child Social Platform) | Ontological alignment triggered "Systemic Benevolence" (Gift Economy, Contextual Safety) vs. Standard "Policing". |

---

*For more information on the Light Framework Kernel, visit the main repository.*
